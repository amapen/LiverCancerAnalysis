{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "arctic-helena",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "import shap\n",
    "import pickle\n",
    "import optuna\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import xgboost as xgb\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.special import ndtri\n",
    "from decimal import Decimal, ROUND_HALF_UP\n",
    "from sklearn.calibration import calibration_curve\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import log_loss, confusion_matrix, roc_auc_score, roc_curve\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_validate, validation_curve, cross_val_score, learning_curve\n",
    "\n",
    "pd.set_option('display.max_columns', 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "powered-intelligence",
   "metadata": {},
   "source": [
    "# データの読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chemical-caution",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('データ抽出.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sought-messenger",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "banned-syndication",
   "metadata": {},
   "source": [
    "# 前処理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exempt-humidity",
   "metadata": {},
   "outputs": [],
   "source": [
    "cut = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "complicated-smoke",
   "metadata": {},
   "source": [
    "## 治療法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "promotional-recruitment",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['治療法解析用'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "southern-hindu",
   "metadata": {},
   "outputs": [],
   "source": [
    "cut += df['治療法解析用'].isnull().sum()\n",
    "df['治療法解析用'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "possible-heart",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna(subset=['治療法解析用'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "popular-henry",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.get_dummies(df, columns=['治療法解析用'], prefix='', prefix_sep='')\n",
    "df = df.drop(columns='無治療')\n",
    "df.rename(columns={'化学療法': 'MTA', '放射線治療': 'Radiation'}, inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extended-expansion",
   "metadata": {},
   "source": [
    "## 前回治療からの期間"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "informational-aspect",
   "metadata": {},
   "outputs": [],
   "source": [
    "cut += df['Last_Treatment'].isnull().sum()\n",
    "df['Last_Treatment'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "linear-reservoir",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Last_Treatment'] = df['Last_Treatment'].replace('#NUM!', 0).replace(0, 10000).astype(int)\n",
    "df['Last_Treatment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pharmaceutical-harrison",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Last_Treatment'] = np.log10(df['Last_Treatment'] + 1)\n",
    "df['Last_Treatment'].hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "level-masters",
   "metadata": {},
   "source": [
    "## 年齢"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vocal-sharing",
   "metadata": {},
   "outputs": [],
   "source": [
    "cut += df['Age'].isnull().sum()\n",
    "df['Age'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "metallic-crowd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna(subset=['Age'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acting-allowance",
   "metadata": {},
   "source": [
    "## 性別"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accepting-tolerance",
   "metadata": {},
   "outputs": [],
   "source": [
    "cut += df['Gender'].isnull().sum()\n",
    "df['Gender'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "outer-musician",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Gender'] = df['Gender'].replace(1,  0).replace(2, 1)\n",
    "df['Gender'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "turned-suspect",
   "metadata": {},
   "source": [
    "## BMI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rising-accused",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['BMI'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "romantic-salon",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['BMI'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "engaging-blind",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['BMI'] = df['BMI'].fillna(df['BMI'].mean())\n",
    "df['BMI'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "designed-mother",
   "metadata": {},
   "source": [
    "## 手術回数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "encouraging-compiler",
   "metadata": {},
   "outputs": [],
   "source": [
    "cut += df['No_of_Admission'].isnull().sum()\n",
    "df['No_of_Admission'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cognitive-shock",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna(subset=['No_of_Admission'])\n",
    "df['No_of_Admission'] = df['No_of_Admission'].astype(int)\n",
    "df['No_of_Admission'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "technological-oregon",
   "metadata": {},
   "source": [
    "## 個数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opened-reconstruction",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['HCC_No'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "domestic-hacker",
   "metadata": {},
   "outputs": [],
   "source": [
    "cut += df['HCC_No'].isnull().sum()\n",
    "df['HCC_No'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "silent-wales",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df['HCC_No'] = df['HCC_No'].fillna(5).astype(int)\n",
    "df = df.dropna(subset=['HCC_No'])\n",
    "df['HCC_No'] = df['HCC_No'].astype(int)\n",
    "df['HCC_No'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "amino-turkish",
   "metadata": {},
   "outputs": [],
   "source": [
    "before = 0\n",
    "l = []\n",
    "for i, n in zip(df['Code'], df['HCC_No']):\n",
    "    if i == before:\n",
    "        l.append(l[-1] + n)\n",
    "    else:\n",
    "        l.append(n)\n",
    "        before = i\n",
    "\n",
    "df['No_Cumsum'] = l\n",
    "df['No_Cumsum'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "smoking-oxide",
   "metadata": {},
   "source": [
    "## サイズ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "attached-patient",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['HCC_size'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "civilian-shirt",
   "metadata": {},
   "outputs": [],
   "source": [
    "cut += df['HCC_size'].isnull().sum()\n",
    "df['HCC_size'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "harmful-photograph",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.replace('diffuse', '1')\n",
    "df = df.dropna(subset=['HCC_size'])\n",
    "df['HCC_size'] = df['HCC_size'].map(lambda x: int(Decimal(str(x)).quantize(Decimal('0'), rounding=ROUND_HALF_UP)))\n",
    "df['HCC_size'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "southern-consultation",
   "metadata": {},
   "source": [
    "## サイズ*個数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "blocked-border",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['NoSize'] = df['HCC_No'] * df['HCC_size']\n",
    "df['NoSize'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stuffed-australia",
   "metadata": {},
   "outputs": [],
   "source": [
    "cut += df['NoSize'].isnull().sum()\n",
    "df['NoSize'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lonely-plaza",
   "metadata": {},
   "outputs": [],
   "source": [
    "before = 0\n",
    "l = []\n",
    "for i, n in zip(df['Code'], df['NoSize']):\n",
    "    if i == before:\n",
    "        l.append(l[-1] + n)\n",
    "    else:\n",
    "        l.append(n)\n",
    "        before = i\n",
    "\n",
    "l_10 = [i//10 for i in l]\n",
    "df['NoSize_Cumsum'] = l_10\n",
    "df['NoSize_Cumsum'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "capital-study",
   "metadata": {},
   "source": [
    "## PS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alternative-characterization",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['PS'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "small-toilet",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PSは0埋め\n",
    "df['PS'] = df['PS'].fillna(0).astype(int)\n",
    "df['PS_Raw'] = df['PS']\n",
    "df = pd.get_dummies(df, columns=['PS'])\n",
    "df = df.drop(columns='PS_0')\n",
    "df['PS_Raw'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "distinct-plumbing",
   "metadata": {},
   "source": [
    "## ALBI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "removed-restriction",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['ALBI_score'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "regional-throat",
   "metadata": {},
   "outputs": [],
   "source": [
    "cut += df['ALBI_score'].isnull().sum()\n",
    "df['ALBI_score'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "changing-backup",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna(subset=['ALBI_score'])\n",
    "df['ALBI_score'] = df['ALBI_score'].map(lambda x: int(Decimal(str(x*(-100))).quantize(Decimal('0'), rounding=ROUND_HALF_UP)))\n",
    "df['ALBI_score'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "composite-scott",
   "metadata": {},
   "source": [
    "## ALBI_grade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hourly-mitchell",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['ALBI_grade'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "annoying-nurse",
   "metadata": {},
   "outputs": [],
   "source": [
    "cut += df['ALBI_grade'].isnull().sum()\n",
    "df['ALBI_grade'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hundred-bibliography",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['ALBI_grade'] = df['ALBI_grade'].replace('3', '4').replace('2b', '3').replace('2a', '2').astype(int)\n",
    "df = pd.get_dummies(df, columns=['ALBI_grade'])\n",
    "df = df.drop(columns='ALBI_grade_1')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "following-quarterly",
   "metadata": {},
   "source": [
    "## AFP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "genuine-briefing",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cut += df['AFP'].isnull().sum()\n",
    "df['AFP'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "typical-prairie",
   "metadata": {},
   "outputs": [],
   "source": [
    "#AFPは0埋め\n",
    "df['AFP'] = df['AFP'].fillna(0).astype(float)\n",
    "df.insert(loc=0, column='AFP_100', value= -1)\n",
    "df.loc[df['AFP'] < 100, 'AFP_100'] = 0\n",
    "df.loc[~(df['AFP'] < 100), 'AFP_100'] = 1\n",
    "df['AFP_100'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recorded-switzerland",
   "metadata": {},
   "source": [
    "## L3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "perceived-broad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cut += df['L3'].isnull().sum()\n",
    "df['L3'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alpha-reset",
   "metadata": {},
   "outputs": [],
   "source": [
    "#L3は0埋め\n",
    "df['L3'] = df['L3'].fillna(0).astype(float)\n",
    "df.insert(loc=0, column='L3_10', value= -1)\n",
    "df.loc[df['L3'] < 10, 'L3_10'] = 0\n",
    "df.loc[~(df['L3'] < 10), 'L3_10'] = 1\n",
    "df['L3_10'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tough-extraction",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['L3_10'] = df['L3_10'].fillna(0).astype(int)\n",
    "df['L3_10'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "possible-avatar",
   "metadata": {},
   "source": [
    "## PIVKA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chemical-shade",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cut += df['PIVKA'].isnull().sum()\n",
    "df['PIVKA'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "large-confidentiality",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PIVKAは0埋め\n",
    "df['PIVKA'] = df['PIVKA'].fillna(0).astype(float)\n",
    "df.insert(loc=0, column='PIVKA_100', value= -1)\n",
    "df.loc[df['PIVKA'] < 100, 'PIVKA_100'] = 0\n",
    "df.loc[~(df['PIVKA'] < 100), 'PIVKA_100'] = 1\n",
    "df['PIVKA_100'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reliable-nature",
   "metadata": {},
   "source": [
    "## Vp_grade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "assumed-relationship",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Vp_grade'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reported-contents",
   "metadata": {},
   "outputs": [],
   "source": [
    "cut += df['Vp_grade'].isnull().sum()\n",
    "df['Vp_grade'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "derived-contact",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Vp_grade'] = df['Vp_grade'].replace(2,  1).replace(3, 1).replace(4, 1)\n",
    "df['Vp_grade'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "optical-magnitude",
   "metadata": {},
   "source": [
    "## Meta0or1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beginning-residence",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Meta0or1'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "small-conditioning",
   "metadata": {},
   "outputs": [],
   "source": [
    "cut += df['Meta0or1'].isnull().sum()\n",
    "df['Meta0or1'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "returning-cement",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna(subset=['Meta0or1'])\n",
    "df['Meta0or1'] = df['Meta0or1'].replace(2, 1).astype(int)\n",
    "df['Meta0or1'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wound-share",
   "metadata": {},
   "source": [
    "## etiology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "refined-circular",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['etiology_C1B2BC3Alc4NBNC5'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "corrected-placement",
   "metadata": {},
   "outputs": [],
   "source": [
    "cut += df['etiology_C1B2BC3Alc4NBNC5'].isnull().sum()\n",
    "df['etiology_C1B2BC3Alc4NBNC5'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "light-affair",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna(subset=['etiology_C1B2BC3Alc4NBNC5'])\n",
    "df = df.rename(columns={'etiology_C1B2BC3Alc4NBNC5': 'etiology_class'})\n",
    "df['etiology_class'] = df['etiology_class'].replace(1,  'C').replace(2, 'B').replace(3, 'BC').replace(4, 'Alc').replace(5, 'NBNC')\n",
    "df['etiology_class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "commercial-reasoning",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.get_dummies(df, columns=['etiology_class'])\n",
    "df.loc[df['etiology_class_BC'] == 1, 'etiology_class_B'] = 1\n",
    "df.loc[df['etiology_class_BC'] == 1, 'etiology_class_C'] = 1\n",
    "df = df.drop(columns=['etiology_class_BC', 'etiology_class_NBNC'])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "based-wrestling",
   "metadata": {},
   "source": [
    "## OS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dried-launch",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['OS_day'] = df['OS_day'].replace('#VALUE!', np.nan).replace('#REF!', np.nan)\n",
    "cut += df['OS_day'].isnull().sum()\n",
    "df['OS_day'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nasty-research",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna(subset=['OS_day'])\n",
    "df['OS_day'] = df['OS_day'].astype(int)\n",
    "df['OS_day'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nonprofit-migration",
   "metadata": {},
   "source": [
    "## 肝臓がんのみを抽出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "polar-bidder",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df['肝癌症例'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "egyptian-galaxy",
   "metadata": {},
   "outputs": [],
   "source": [
    "cut += len(df[df['肝癌症例']==0])\n",
    "len(df[df['肝癌症例']==0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "standard-climate",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['肝癌症例'] == 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "approved-diabetes",
   "metadata": {},
   "source": [
    "## dfとcutの確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lyric-snapshot",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 肝臓がんのみを抽出\n",
    "\n",
    "df['肝癌症例'].value_counts()\n",
    "\n",
    "cut += len(df[df['肝癌症例']==0])\n",
    "len(df[df['肝癌症例']==0])\n",
    "\n",
    "df = df[df['肝癌症例'] == 1]\n",
    "\n",
    "## dfとcutの確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "documentary-throw",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "available-singles",
   "metadata": {},
   "outputs": [],
   "source": [
    "cut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "signal-accessory",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df[df['No_of_Admission'] == 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incorporate-booking",
   "metadata": {},
   "source": [
    "## 3yearの作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pointed-diversity",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Death1Alive0'] = df['Death1Alive0'].astype(int)\n",
    "df.insert(loc=0, column='3year', value= -1)\n",
    "df.loc[(df['OS_day'] < 1095) & (df['Death1Alive0'] == 1), '3year'] = 0\n",
    "df.loc[df['OS_day'] >= 1095, '3year'] = 1\n",
    "df['3year'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "explicit-winner",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 3年後の生死が未確認（OS.day<1095&Death1Alive0=0)を削除\n",
    "df = df[df['3year'] != -1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "leading-milan",
   "metadata": {},
   "source": [
    "## 学習データ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conceptual-telescope",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data = df.loc[:,['Ablation', 'OPE', 'TAE', 'MTA', 'Radiation', 'Last_Treatment', 'Age', 'Gender', 'BMI', 'No_of_Admission', 'HCC_No', 'No_Cumsum', \n",
    "                 'HCC_size', 'NoSize', 'NoSize_Cumsum', 'PS_Raw', 'PS_1', 'PS_2', 'PS_3', 'PS_4', 'ALBI_score', 'AFP_100', 'L3_10', 'PIVKA_100', \n",
    "                 'Vp_grade', 'Meta0or1', 'etiology_class_C', 'etiology_class_B', 'etiology_class_Alc']]\n",
    "target = df['3year']\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cross-prisoner",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "separated-logan",
   "metadata": {},
   "source": [
    "# TrainとValidの作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "palestinian-riding",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_valid, y_train, y_valid = train_test_split(data, target, train_size = 0.8, random_state = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "molecular-karen",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "atlantic-organ",
   "metadata": {},
   "source": [
    "# GBDT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "worthy-agent",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop(['PS_1', 'PS_2', 'PS_3', 'PS_4'], axis=1)\n",
    "Y = target.values.ravel()\n",
    "\n",
    "model = xgb.XGBClassifier(objective='binary:logistic', eval_metric='logloss', eta=0.05, max_depth=4, use_label_encoder=False)\n",
    "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "results = cross_validate(model, X, Y, scoring=['neg_log_loss', 'roc_auc'], cv=kfold)\n",
    "\n",
    "print(results)\n",
    "print()\n",
    "print('LogLoss :', np.mean(results['test_neg_log_loss'])*-1)\n",
    "print('AUC :', np.mean(results['test_roc_auc']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conceptual-graduate",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_params = {'subsample': [0, 0.1, 0.2, 0.3, 0.4, 0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "             'colsample_bytree': [0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "             'reg_alpha': [0, 0.0001, 0.001, 0.01, 0.03, 0.1, 0.3, 1.0, 1.5, 2.0],\n",
    "             'reg_lambda': [0, 0.0001, 0.001, 0.01, 0.03, 0.1, 0.3, 1.0, 1.5, 2.0],\n",
    "             'learning_rate': [0, 0.0001, 0.001, 0.01, 0.03, 0.1, 0.3, 1.0],\n",
    "             'min_child_weight': [1, 3, 5, 7, 9, 11, 13, 15, 18],\n",
    "             'max_depth': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
    "             'gamma': [0, 0.0001, 0.001, 0.01, 0.03, 0.1, 0.3, 1.0, 1.5, 2.0]\n",
    "             }\n",
    "param_scales = {'subsample': 'linear',\n",
    "                'colsample_bytree': 'linear',\n",
    "                'reg_alpha': 'log',\n",
    "                'reg_lambda': 'log',\n",
    "                'learning_rate': 'log',\n",
    "                'min_child_weight': 'linear',\n",
    "                'max_depth': 'linear',\n",
    "                'gamma': 'log'\n",
    "                }\n",
    "fit_params = {'verbose': 0,  'early_stopping_rounds': 20,  'eval_metric': 'logloss',  'eval_set': [(X, Y)]}\n",
    "scoring = 'neg_log_loss'\n",
    "\n",
    "# 検証曲線のプロット（パラメータ毎にプロット）\n",
    "for i, (k, v) in enumerate(cv_params.items()):\n",
    "    train_scores, valid_scores = validation_curve(estimator=model,\n",
    "                                                  X=X, y=Y,\n",
    "                                                  param_name=k,\n",
    "                                                  param_range=v,\n",
    "                                                  fit_params=fit_params,\n",
    "                                                  cv=kfold, scoring=scoring,\n",
    "                                                  n_jobs=-1)\n",
    "    # 学習データに対するスコアの平均±標準偏差を算出\n",
    "    train_mean = np.mean(train_scores, axis=1)\n",
    "    train_std  = np.std(train_scores, axis=1)\n",
    "    train_center = train_mean\n",
    "    train_high = train_mean + train_std\n",
    "    train_low = train_mean - train_std\n",
    "    # テストデータに対するスコアの平均±標準偏差を算出\n",
    "    valid_mean = np.mean(valid_scores, axis=1)\n",
    "    valid_std  = np.std(valid_scores, axis=1)\n",
    "    valid_center = valid_mean\n",
    "    valid_high = valid_mean + valid_std\n",
    "    valid_low = valid_mean - valid_std\n",
    "    # training_scoresをプロット\n",
    "    plt.plot(v, train_center, color='blue', marker='o', markersize=5, label='training score')\n",
    "    plt.fill_between(v, train_high, train_low, alpha=0.15, color='blue')\n",
    "    # validation_scoresをプロット\n",
    "    plt.plot(v, valid_center, color='green', linestyle='--', marker='o', markersize=5, label='validation score')\n",
    "    plt.fill_between(v, valid_high, valid_low, alpha=0.15, color='green')\n",
    "    # スケールをparam_scalesに合わせて変更\n",
    "    plt.xscale(param_scales[k])\n",
    "    # 軸ラベルおよび凡例の指定\n",
    "    plt.xlabel(k)  # パラメータ名を横軸ラベルに\n",
    "    plt.ylabel(scoring)  # スコア名を縦軸ラベルに\n",
    "    plt.legend(loc='lower right')  # 凡例\n",
    "    # グラフを描画\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unlikely-popularity",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "# ベイズ最適化時の評価指標算出メソッド\n",
    "def bayes_objective(trial):\n",
    "    params = {\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.5, log=True),\n",
    "        'min_child_weight': trial.suggest_int('min_child_weight', 2, 8),\n",
    "        'max_depth': trial.suggest_int('max_depth', 4, 10),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.2, 1.0),\n",
    "        'subsample': trial.suggest_float('subsample', 0.2, 1.0),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 0.0001, 0.1, log=True),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 0.0001, 0.1, log=True),\n",
    "        'gamma': trial.suggest_float('gamma', 0.0001, 0.1, log=True),\n",
    "    }\n",
    "    # モデルにパラメータ適用\n",
    "    model.set_params(**params)\n",
    "    # cross_val_scoreでクロスバリデーション\n",
    "    scores = cross_val_score(model, X, Y, cv=kfold,\n",
    "                             scoring=scoring, fit_params=fit_params, n_jobs=-1)\n",
    "    val = scores.mean()\n",
    "    return val\n",
    "\n",
    "# ベイズ最適化を実行\n",
    "study = optuna.create_study(direction='maximize',\n",
    "                            sampler=optuna.samplers.TPESampler(seed=42))\n",
    "study.optimize(bayes_objective, n_trials=600)\n",
    "\n",
    "# 最適パラメータの表示と保持\n",
    "best_params = study.best_trial.params\n",
    "best_score = study.best_trial.value\n",
    "print(f'最適パラメータ {best_params}\\nスコア {best_score}')\n",
    "print(f'所要時間{time.time() - start}秒')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conditional-amendment",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import learning_curve\n",
    "import matplotlib.pyplot as plt\n",
    "# 最適パラメータを学習器にセット\n",
    "model.set_params(**best_params)\n",
    "\n",
    "# 学習曲線の取得\n",
    "train_sizes, train_scores, valid_scores = learning_curve(estimator=model,\n",
    "                                                         X=X, y=Y,\n",
    "                                                         train_sizes=np.linspace(0.1, 1.0, 10),\n",
    "                                                         fit_params=fit_params,\n",
    "                                                         cv=kfold, scoring=scoring, n_jobs=-1)\n",
    "# 学習データ指標の平均±標準偏差を計算\n",
    "train_mean = np.mean(train_scores, axis=1)\n",
    "train_std  = np.std(train_scores, axis=1)\n",
    "train_center = train_mean\n",
    "train_high = train_mean + train_std\n",
    "train_low = train_mean - train_std\n",
    "# 検証データ指標の平均±標準偏差を計算\n",
    "valid_mean = np.mean(valid_scores, axis=1)\n",
    "valid_std  = np.std(valid_scores, axis=1)\n",
    "valid_center = valid_mean\n",
    "valid_high = valid_mean + valid_std\n",
    "valid_low = valid_mean - valid_std\n",
    "# training_scoresをプロット\n",
    "plt.plot(train_sizes, train_center, color='blue', marker='o', markersize=5, label='training score')\n",
    "plt.fill_between(train_sizes, train_high, train_low, alpha=0.15, color='blue')\n",
    "# validation_scoresをプロット\n",
    "plt.plot(train_sizes, valid_center, color='green', linestyle='--', marker='o', markersize=5, label='validation score')\n",
    "plt.fill_between(train_sizes, valid_high, valid_low, alpha=0.15, color='green')\n",
    "# 最高スコアの表示\n",
    "best_score = valid_center[len(valid_center) - 1]\n",
    "plt.text(np.amax(train_sizes), valid_low[len(valid_low) - 1], f'best_score={best_score}',\n",
    "                color='black', verticalalignment='top', horizontalalignment='right')\n",
    "# 軸ラベルおよび凡例の指定\n",
    "plt.xlabel('training examples')  # 学習サンプル数を横軸ラベルに\n",
    "plt.ylabel(scoring)  # スコア名を縦軸ラベルに\n",
    "plt.legend(loc='lower right')  # 凡例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coupled-aberdeen",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_curve_params = {'subsample': [0, 0.1, 0.2, 0.3, 0.4, 0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "             'colsample_bytree': [0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "             'reg_alpha': [0, 0.0001, 0.001, 0.01, 0.03, 0.1, 0.3, 1.0, 1.5, 2.0],\n",
    "             'reg_lambda': [0, 0.0001, 0.001, 0.01, 0.03, 0.1, 0.3, 1.0, 1.5, 2.0],\n",
    "             'learning_rate': [0, 0.0001, 0.001, 0.01, 0.03, 0.1, 0.3, 1.0],\n",
    "             'min_child_weight': [1, 3, 5, 7, 9, 11, 13, 15, 18],\n",
    "             'max_depth': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
    "             'gamma': [0, 0.0001, 0.001, 0.01, 0.03, 0.1, 0.3, 1.0, 1.5, 2.0]\n",
    "             }\n",
    "\n",
    "for k, v in valid_curve_params.items():\n",
    "    if best_params[k] not in v:\n",
    "        v.append(best_params[k])\n",
    "        v.sort()\n",
    "for i, (k, v) in enumerate(valid_curve_params.items()):\n",
    "    # モデルに最適パラメータを適用\n",
    "    model.set_params(**best_params)\n",
    "    # 検証曲線を描画\n",
    "    train_scores, valid_scores = validation_curve(estimator=model,\n",
    "                                                  X=X, y=Y,\n",
    "                                                  param_name=k,\n",
    "                                                  param_range=v,\n",
    "                                                  fit_params=fit_params,\n",
    "                                                  cv=kfold, scoring=scoring,\n",
    "                                                  n_jobs=-1)\n",
    "    # 学習データに対するスコアの平均±標準偏差を算出\n",
    "    train_mean = np.mean(train_scores, axis=1)\n",
    "    train_std  = np.std(train_scores, axis=1)\n",
    "    train_center = train_mean\n",
    "    train_high = train_mean + train_std\n",
    "    train_low = train_mean - train_std\n",
    "    # テストデータに対するスコアの平均±標準偏差を算出\n",
    "    valid_mean = np.mean(valid_scores, axis=1)\n",
    "    valid_std  = np.std(valid_scores, axis=1)\n",
    "    valid_center = valid_mean\n",
    "    valid_high = valid_mean + valid_std\n",
    "    valid_low = valid_mean - valid_std\n",
    "    # training_scoresをプロット\n",
    "    plt.plot(v, train_center, color='blue', marker='o', markersize=5, label='training score')\n",
    "    plt.fill_between(v, train_high, train_low, alpha=0.15, color='blue')\n",
    "    # validation_scoresをプロット\n",
    "    plt.plot(v, valid_center, color='green', linestyle='--', marker='o', markersize=5, label='validation score')\n",
    "    plt.fill_between(v, valid_high, valid_low, alpha=0.15, color='green')\n",
    "    # 最適パラメータを縦線表示\n",
    "    plt.axvline(x=best_params[k], color='gray')\n",
    "    # スケールをparam_scalesに合わせて変更\n",
    "    plt.xscale(param_scales[k])\n",
    "    # 軸ラベルおよび凡例の指定\n",
    "    plt.xlabel(k)  # パラメータ名を横軸ラベルに\n",
    "    plt.ylabel(scoring)  # スコア名を縦軸ラベルに\n",
    "    plt.legend(loc='lower right')  # 凡例\n",
    "    # グラフを描画\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "direct-northern",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_gbdt = x_train.drop(['PS_1', 'PS_2', 'PS_3', 'PS_4'], axis=1)\n",
    "x_valid_gbdt = x_valid.drop(['PS_1', 'PS_2', 'PS_3', 'PS_4'], axis=1)\n",
    "\n",
    "dtrain = xgb.DMatrix(x_train_gbdt, label=y_train)\n",
    "dvalid = xgb.DMatrix(x_valid_gbdt, label=y_valid)\n",
    "x_train_gbdt.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "medieval-share",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = {'learning_rate': 0.08566665798184023, 'min_child_weight': 2, 'max_depth': 6, 'colsample_bytree': 0.6332864539990984, \n",
    "               'subsample': 0.8749273245384632, 'reg_alpha': 0.010240948422244883, 'reg_lambda': 0.0006712774237446539, 'gamma': 0.013301717822084034}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "russian-confirmation",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "params = {'objective': 'binary:logistic', 'eval_metric': 'logloss', 'booster': 'gbtree'} | best_params\n",
    "num_round = 1000\n",
    "watchlist = [(dtrain, 'train'), (dvalid, 'eval')]\n",
    "evals_result = {}\n",
    "model_GBDT = xgb.train(params, dtrain, num_round, early_stopping_rounds=20, evals=watchlist, evals_result=evals_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sorted-trademark",
   "metadata": {},
   "outputs": [],
   "source": [
    "#trainデータに対してのloss推移をplot\n",
    "plt.plot(evals_result['train']['logloss'], label='train logloss')\n",
    "#validデータに対してのloss推移をplot\n",
    "plt.plot(evals_result['eval']['logloss'], label='eval logloss')\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.xlabel('rounds')\n",
    "plt.ylabel('logloss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expired-virus",
   "metadata": {},
   "outputs": [],
   "source": [
    "git, ax = plt.subplots(figsize=(12, 12))\n",
    "xgb.plot_importance(model_GBDT, height=0.8, ax=ax)\n",
    "#plt.savefig('1021_Feature_Importance_3year.jpg', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "outside-charleston",
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.TreeExplainer(model_GBDT)\n",
    "shap_values = explainer.shap_values(x_train_gbdt)\n",
    "shap.summary_plot(shap_values, x_train_gbdt, max_display=100, show=False)\n",
    "plt.title('SHAP_Value_3year.jpg')\n",
    "#plt.savefig('results/0111_Feature_Importance_3year_SHAP.jpg', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "together-contents",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "amber-cycling",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_GBDT = model_GBDT.predict(dvalid)\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.ylabel(\"Predict\")\n",
    "plt.xlabel(\"Actual\")\n",
    "plt.xlim(0, 1)\n",
    "plt.scatter(pred_GBDT, y_valid)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chicken-robertson",
   "metadata": {},
   "outputs": [],
   "source": [
    "zero, one = [], []\n",
    "\n",
    "for i in range(len(pred_GBDT)):\n",
    "    if y_valid.get(i) == 0:\n",
    "        zero.append(pred_GBDT[i])\n",
    "    else:\n",
    "        one.append(pred_GBDT[i])\n",
    "        \n",
    "plt.figure(figsize=(10, 6))\n",
    "bins = np.linspace(0, 1, 50)\n",
    "\n",
    "plt.hist(zero, bins, alpha = 0.5, label='zero', density = True)\n",
    "plt.hist(one, bins, alpha = 0.5, label='one', density = True)\n",
    "plt.legend(loc='upper left')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unlimited-attachment",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# ROC曲線の値の生成：fpr、tpr、閾値\n",
    "fpr, tpr, thresholds = roc_curve(y_valid, pred_GBDT)\n",
    "\n",
    "# ROC曲線のプロット\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.plot(fpr, tpr, label='Logistic Regression')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Training_Cohort_ROC_3year')\n",
    "#plt.savefig('results/0111_Valid_ROC_3year.jpg', dpi=300)\n",
    "plt.show()\n",
    "\n",
    "#AUCの表示\n",
    "auc_GBDT = roc_auc_score(y_valid, pred_GBDT)\n",
    "print(auc_GBDT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lesbian-lithuania",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(y_valid.values.tolist()[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "identical-silly",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.round(pred_GBDT).astype(int).tolist()[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accomplished-oakland",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_valid.values.tolist(), np.round(pred_GBDT).astype(int).tolist())\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "needed-slave",
   "metadata": {},
   "outputs": [],
   "source": [
    "def roc_auc_ci(y_true, y_score, positive=1):\n",
    "    AUC = roc_auc_score(y_true, y_score)\n",
    "    N1 = sum(y_true == positive)\n",
    "    N2 = sum(y_true != positive)\n",
    "    Q1 = AUC / (2 - AUC)\n",
    "    Q2 = 2*AUC**2 / (1 + AUC)\n",
    "    SE_AUC = math.sqrt((AUC*(1 - AUC) + (N1 - 1)*(Q1 - AUC**2) + (N2 - 1)*(Q2 - AUC**2)) / (N1*N2))\n",
    "    lower = AUC - 1.96*SE_AUC\n",
    "    upper = AUC + 1.96*SE_AUC\n",
    "    if lower < 0:\n",
    "        lower = 0\n",
    "    if upper > 1:\n",
    "        upper = 1\n",
    "    return (lower, upper)\n",
    "\n",
    "roc_auc_ci(y_valid, pred_GBDT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abstract-gauge",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _proportion_confidence_interval(r, n, z):\n",
    "    A = 2*r + z**2\n",
    "    B = z*math.sqrt(z**2 + 4*r*(1 - r/n))\n",
    "    C = 2*(n + z**2)\n",
    "    return ((A-B)/C, (A+B)/C)\n",
    "\n",
    "def sensitivity_and_specificity_with_confidence_intervals(TP, FP, FN, TN, alpha):\n",
    "    z = -ndtri((1.0-alpha)/2)\n",
    "    \n",
    "    # Compute sensitivity using method described in [1]\n",
    "    sensitivity_point_estimate = TP/(TP + FN)\n",
    "    sensitivity_confidence_interval = _proportion_confidence_interval(TP, TP + FN, z)\n",
    "    \n",
    "    # Compute specificity using method described in [1]\n",
    "    specificity_point_estimate = TN/(TN + FP)\n",
    "    specificity_confidence_interval = _proportion_confidence_interval(TN, TN + FP, z)\n",
    "    \n",
    "    return sensitivity_point_estimate, specificity_point_estimate, sensitivity_confidence_interval, specificity_confidence_interval\n",
    "\n",
    "sensitivity_and_specificity_with_confidence_intervals(cm[1][1], cm[0][1], cm[1][0], cm[0][0], 0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "worldwide-tract",
   "metadata": {},
   "outputs": [],
   "source": [
    "def viz_calibration_curve(y_test, y_pred, name):\n",
    "    frac_of_pos, mean_pred_value = calibration_curve(y_test, y_pred, n_bins=10)\n",
    "\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(15,6))\n",
    "    ax[0].plot([0, 1], [0, 1], \"k:\", label=\"Perfectly calibrated\")\n",
    "    ax[0].plot(mean_pred_value, frac_of_pos, marker=\"o\", label=f'{name}')\n",
    "    ax[0].set_ylabel(\"Fraction of positives\")\n",
    "    ax[0].set_ylim([-0.05, 1.05])\n",
    "    ax[0].legend(loc=\"lower right\")\n",
    "    ax[0].set_title(f'Calibration plot 3year ({name})')\n",
    "    \n",
    "    sns.distplot(y_pred, bins=100, label='predicted score', ax=ax[1])\n",
    "    ax[1].legend(loc='upper right')\n",
    "    ax[1].set_xlim([-0.05, 1.05])\n",
    "    #plt.savefig('results/0111_calibration_3year.jpg', dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "# AUCとReliability Diagramの可視化\n",
    "viz_calibration_curve(y_valid, pred_GBDT, 'XGBoost')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "painful-introduction",
   "metadata": {},
   "source": [
    "# NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hundred-colonial",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "x_train_nn = x_train.drop(['PS_Raw'], axis=1)\n",
    "x_valid_nn = x_valid.drop(['PS_Raw'], axis=1)\n",
    "\n",
    "x_train_nn = scaler.fit_transform(x_train_nn)\n",
    "x_valid_nn = scaler.fit_transform(x_valid_nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "electrical-melbourne",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_NN = tf.keras.models.Sequential()\n",
    "model_NN.add(tf.keras.layers.Dense(16, activation='relu', input_shape=(x_train_nn.shape[1],)))\n",
    "model_NN.add(tf.keras.layers.Dropout(0.1))\n",
    "model_NN.add(tf.keras.layers.Dense(8, activation='relu'))\n",
    "model_NN.add(tf.keras.layers.Dropout(0.1))\n",
    "model_NN.add(tf.keras.layers.Dense(8, activation='relu'))\n",
    "model_NN.add(tf.keras.layers.Dropout(0.1))\n",
    "model_NN.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(0.001)\n",
    "\n",
    "model_NN.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mighty-signature",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_NN.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "outer-prerequisite",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "epochs = 1000\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n",
    "\n",
    "history_NN = model_NN.fit(x_train_nn, y_train, batch_size=batch_size, epochs=epochs, verbose=1, \n",
    "                          validation_data = (x_valid_nn, y_valid), callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fabulous-arizona",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = pd.DataFrame(history_NN.history)\n",
    "hist['epoch'] = history_NN.epoch\n",
    "hist.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "attached-exploration",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history):\n",
    "    hist = pd.DataFrame(history.history)\n",
    "    hist['epoch'] = history.epoch\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.plot(hist['epoch'], hist['accuracy'], label='Train Accuracy')\n",
    "    plt.plot(hist['epoch'], hist['val_accuracy'], label = 'Val Accuracy')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.plot(hist['epoch'], hist['loss'], label='Train Loss')\n",
    "    plt.plot(hist['epoch'], hist['val_loss'], label = 'Val Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "plot_history(history_NN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hawaiian-metadata",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pred_NN = model_NN.predict(x_valid_nn)\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.xlabel(\"Predict\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.xlim(0, 1)\n",
    "plt.scatter(pred_NN, y_valid)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unlimited-genetics",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_valid_array = np.array(y_valid).reshape(-1, 1)\n",
    "y_valid_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affiliated-emerald",
   "metadata": {},
   "outputs": [],
   "source": [
    "zero, one = [], []\n",
    "\n",
    "for i in range(len(pred_NN)):\n",
    "    if y_valid_array[i][0] == 0:\n",
    "        zero.append(pred_NN[i][0])\n",
    "    else:\n",
    "        one.append(pred_NN[i][0])\n",
    "        \n",
    "plt.figure(figsize=(10, 6))\n",
    "bins = np.linspace(0, 1, 50)\n",
    "\n",
    "plt.hist(zero, bins, alpha = 0.5, label='Dead', density = True)\n",
    "plt.hist(one, bins, alpha = 0.5, label='Alive', density = True)\n",
    "plt.legend(loc='upper left')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "romance-criminal",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC曲線の値の生成：fpr、tpr、閾値\n",
    "fpr, tpr, thresholds = roc_curve(y_valid, pred_NN)\n",
    "\n",
    "# ROC曲線のプロット\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.plot(fpr, tpr, label='Logistic Regression')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('1021_Valid_ROC_3year')\n",
    "#plt.savefig(\"1021_Valid_ROC_3year.jpg\", dpi=300)\n",
    "plt.show()\n",
    "\n",
    "#AUCの表示\n",
    "auc_NN = roc_auc_score(y_valid, pred_NN)\n",
    "print(auc_NN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "announced-southwest",
   "metadata": {},
   "source": [
    "# Total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "separate-victim",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_GBDT = pred_GBDT.reshape(-1, 1)\n",
    "pred_GBDT.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "minimal-currency",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_NN.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accepting-blood",
   "metadata": {},
   "outputs": [],
   "source": [
    "logloss_total = log_loss(y_valid, (pred_GBDT+pred_NN)/2)\n",
    "logloss_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pressing-marine",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_total = (pred_NN+pred_GBDT)/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spanish-proportion",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 6))\n",
    "plt.ylabel(\"Predict\")\n",
    "plt.xlabel(\"Actual\")\n",
    "plt.xlim(0, 1)\n",
    "plt.scatter(pred_total, y_valid)\n",
    "#plt.savefig(\"0716_Predict_Actual_Emsemble.jpg\", dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wooden-execution",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_total[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "other-grounds",
   "metadata": {},
   "outputs": [],
   "source": [
    "zero, one = [], []\n",
    "\n",
    "for i in range(len(pred_total)):\n",
    "    if y_valid_array[i][0] == 0:\n",
    "        zero.append(pred_total[i][0])\n",
    "    else:\n",
    "        one.append(pred_total[i][0])\n",
    "        \n",
    "plt.figure(figsize=(10, 6))\n",
    "bins = np.linspace(0, 1, 50)\n",
    "\n",
    "plt.hist(zero, bins, alpha = 0.5, label='Dead', density = True)\n",
    "plt.hist(one, bins, alpha = 0.5, label='Alive', density = True)\n",
    "plt.legend(loc='upper left')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "immediate-finance",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC曲線の値の生成：fpr、tpr、閾値\n",
    "fpr, tpr, thresholds = roc_curve(y_valid, pred_total)\n",
    "\n",
    "# ROC曲線のプロット\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.plot(fpr, tpr, label='Logistic Regression')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('1021_Valid_ROC_3year')\n",
    "#plt.savefig(\"1021_Valid_ROC_3year.jpg\", dpi=300)\n",
    "plt.show()\n",
    "\n",
    "#AUCの表示\n",
    "auc_total = roc_auc_score(y_valid, pred_total)\n",
    "print(auc_total)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "later-accused",
   "metadata": {},
   "source": [
    "# Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "composed-stroke",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pickle.dump(model_GBDT, open('models/0111_model_GBDT_3year.pickle', 'wb'))\n",
    "#model_NN.save('models/1022_model_NN_3year.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fresh-break",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
